{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from utility import calculate_input_gradients, perturb_inputs, preprocess_images, \\\n",
    "                    postprocess_features, save_data_hdf5,get_dataset_hdf5,\\\n",
    "                    build_one_class_svm, combine_inliners_outliers, apply_temp_scale_to_model,\\\n",
    "                    apply_log_temp_scale_to_model, perturb_inputs_odin, extract_layer_features\n",
    " \n",
    "from utility_db_outliers import load_dataset\n",
    "from models_lib import load_custom_model_for_ds\n",
    "import h5py\n",
    "# from metrics2 import *\n",
    "from metrics import *\n",
    "from general_setting import *\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "#------------\n",
    "from utility_methods import *\n",
    "# from utility_methods2 import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import scipy.io as sio\n",
    "import tensorflow.keras.backend as K\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Lambda\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose, MaxPooling2D, UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 ResNet\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "SAVE_RESULTS = True\n",
    "id_name=ID_DS_LIST[1] # selects the ID dataset.\n",
    "id_model=ID_MODEL_LIST[2]  # select the deep model used for training ID dataset.\n",
    "print(id_name,id_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ResNet-V1-44 model for CIFAR10 was loaded.\n"
     ]
    }
   ],
   "source": [
    "org_model = load_custom_model_for_ds(id_name, id_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Conv2D, Conv2DTranspose, Dense, Flatten,\n",
    "                                     InputLayer, Reshape, Layer, Input)\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "from tensorflow.keras.losses import kld, categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = org_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu',strides=2, padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',strides=2, padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu',strides=2, padding='same'))\n",
    "    model.add(Conv2DTranspose(64, (3, 3), activation='relu',strides=2, padding='same'))\n",
    "    model.add(Conv2DTranspose(32, (3, 3), activation='relu',strides=2, padding='same'))\n",
    "    # model.add(Conv2DTranspose(3, (3, 3), activation='sigmoid',strides=2, padding='same'))\n",
    "\n",
    "    model.add(Conv2DTranspose(3, (3, 3), activation=None,strides=2, padding='same'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = head_model()\n",
    "head_model.add(base_model)\n",
    "model = head_model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaofz/anaconda3/envs/tf-gpu3/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing was done for  CIFAR10 ResNet\n",
      "Preprocessing was done for  CIFAR10 ResNet\n"
     ]
    }
   ],
   "source": [
    "(org_traing_data, org_training_labels),(id_eva_data,org_testing_labels)  = load_dataset(id_name)\n",
    "\n",
    "org_traing_data_processed = preprocess_images(id_name, org_traing_data, id_model, verbose=True)\n",
    "id_eva_data_processed = preprocess_images(id_name, id_eva_data, id_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = org_traing_data_processed\n",
    "y_train = org_training_labels\n",
    "x_test = id_eva_data_processed\n",
    "y_test = org_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = base_model.predict(x_train)\n",
    "print(y_true.shape)\n",
    "np.save('./KL_datas/%s_%s_train_0.npy'%(id_name, id_model), x_train)\n",
    "np.save('./KL_datas/%s_%s_preds_0.npy'%(id_name, id_model), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.load('./KL_datas/%s_%s_train.npy'%(id_name, id_model))\n",
    "y_true = np.load('./KL_datas/%s_%s_preds.npy'%(id_name, id_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = base_model.evaluate(x_test,y_test)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if str(layer.name) == 'sequential':\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(y_true, y_pred):\n",
    "    temperature = 150\n",
    "    y_true = y_true ** (1 / temperature)\n",
    "    loss_kld = kld(y_true, y_pred)\n",
    "    # std_kld = tf.math.reduce_std(loss_kld)\n",
    "    loss = tf.reduce_mean(loss_kld)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_loss_fn, optimizer=Adam(lr=1e-2), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=256, epochs=30)\n",
    "model.save('./KL_models/%s_%s_KL.h5'%(id_name, id_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 3)         867       \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 10)                665994    \n",
      "=================================================================\n",
      "Total params: 852,365\n",
      "Trainable params: 186,371\n",
      "Non-trainable params: 665,994\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./KL_models/%s_%s_KL.h5'%(id_name,id_model), custom_objects={'my_loss_fn': my_loss_fn})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(y_true, y_pred):\n",
    "    temperature = 150\n",
    "    y_true = y_true ** (1 / temperature)\n",
    "    loss_kld = kld(y_true, y_pred)\n",
    "    # std_kld = tf.math.reduce_std(loss_kld)\n",
    "    loss = tf.reduce_mean(loss_kld)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_id = []\n",
    "for i in range(len(x_test)):\n",
    "    x0 = base_model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    x1 = model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    score = mark(x0,x1)\n",
    "    # print(score)\n",
    "    sc_id.append(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inliners_outliers(inliers, outliers, i_label=1, o_label=0, verbose=False):\n",
    "\n",
    "    temp_outliers = outliers\n",
    "    temp_inliers  = inliers\n",
    "    if i_label==1:\n",
    "        i_labels = np.ones(temp_inliers.shape[0])\n",
    "    else:\n",
    "        i_labels = np.zeros(temp_inliers.shape[0])\n",
    "        \n",
    "    if o_label==0:\n",
    "        o_labels = np.zeros(temp_outliers.shape[0])\n",
    "    else:\n",
    "        o_labels = np.ones(temp_outliers.shape[0])       \n",
    "              \n",
    "    mixed_labels =  np.append(i_labels, o_labels)\n",
    "    mixed_data = np.hstack((temp_inliers, temp_outliers))\n",
    "\n",
    "    return mixed_data, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_inx specifies the index of the OODL found by \"find_oodl\" jupyter file.\n",
    "REM_TOP_LAYER = -2\n",
    "NUM_CLASS = 10\n",
    "if id_name==\"MNIST\" and id_model==\"LeNet\":\n",
    "    layer_inx = 0\n",
    "    OOD_DS_LIST       = OOD_DS_LIST_MNIST\n",
    "    #**********************************************************\n",
    "elif id_name==\"CIFAR10\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_CIFAR10\n",
    "    if id_model==\"VGG\":\n",
    "        layer_inx = 6\n",
    "    elif id_model==\"ResNet\":\n",
    "        layer_inx = 20\n",
    "    #**********************************************************\n",
    "elif id_name==\"SVHN\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_SVHN\n",
    "    if id_model==\"VGG\":\n",
    "        layer_inx = 6\n",
    "    elif id_model==\"ResNet\":\n",
    "        layer_inx = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vriance模块(OODL大类别 --> 删除同样的feature maps)\n",
    "test = np.asarray(sc_id)\n",
    "for i, ood_ds_name in enumerate(OOD_DS_LIST):\n",
    "    (_,_),(ood_eva_data,_) = load_dataset(ood_ds_name)\n",
    "    das = preprocess_images(id_name, ood_eva_data, id_model, verbose=True)\n",
    "    \n",
    "    # scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "    \n",
    "    sc_ood = []\n",
    "    for i in range(len(das)):\n",
    "        x0 = base_model.predict(das[i].reshape(1,32,32,3))\n",
    "        x1 = model.predict(das[i].reshape(1,32,32,3))\n",
    "        score = mark(x0,x1)\n",
    "        sc_ood.append(-score)\n",
    "    \n",
    "    ood = np.asarray(sc_ood) \n",
    "    scores, mixed_labels = combine_inliners_outliers(test, ood) \n",
    "\n",
    "    fpr, tpr = nums(scores, mixed_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lens = id_eva_data_processed.shape[0]\n",
    "\n",
    "    FP,TN,TP,FN = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "#     print(get_summary_statistics(scores, mixed_labels))\n",
    "    print('The results of %s:\\n' %(ood_ds_name))\n",
    "#     print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "    print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "    print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "#     print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "    print(\"AUROC: \",ROC*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_ds_name = 'foolingimages'\n",
    "\n",
    "das = np.load('adv_datasets/%s_%s_fooling_images.npy'%(id_name,id_model))\n",
    "\n",
    "# scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "# das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "# das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "\n",
    "sc_ood = []\n",
    "for i in range(len(das)):\n",
    "        x0 = base_model.predict(das[i].reshape(1,32,32,3))\n",
    "        x1 = model.predict(das[i].reshape(1,32,32,3))\n",
    "        score = mark(x0,x1)\n",
    "        sc_ood.append(-score)\n",
    "\n",
    "ood = np.asarray(sc_ood) \n",
    "scores, mixed_labels = combine_inliners_outliers(test, ood) \n",
    "\n",
    "# print('scores is:', (scores[10000:20000]))\n",
    "fpr, tpr = nums(scores, mixed_labels)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "lens = id_eva_data_processed.shape[0]\n",
    "FP,TN,TP,FN = ErrorRateAt95Recall1(lens,scores, mixed_labels)\n",
    "ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "print('The results of %s:\\n' %(ood_ds_name))\n",
    "# print(get_summary_statistics(scores, mixed_labels))\n",
    "# print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "# print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "print(\"AUROC: \",ROC*100)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('./advs_new/%s_%s_normal.npy'%(id_name, id_model),allow_pickle=True)\n",
    "# scaler.fit(x_test.flatten().reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "# x_test_0 = scaler.transform(x_test.flatten().reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "# x_test = x_test_0.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],x_test.shape[3])\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_id = []\n",
    "for i in range(len(x_test)):\n",
    "    x0 = base_model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    x1 = model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    score = mark(x0,x1)\n",
    "    # print(score)\n",
    "    sc_id.append(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of fgsm:\n",
      "\n",
      "tnr_at_95_tpr: 37.298919567379365\n",
      "detection_acc: 66.14945978321467\n",
      "AUROC:  93.24106064218194\n",
      "\n",
      "\n",
      "The results of bim-a:\n",
      "\n",
      "tnr_at_95_tpr: 30.096038415004845\n",
      "detection_acc: 62.548019207027416\n",
      "AUROC:  86.47647050244815\n",
      "\n",
      "\n",
      "The results of bim-b:\n",
      "\n",
      "tnr_at_95_tpr: 0.0\n",
      "detection_acc: 47.499999999524995\n",
      "AUROC:  0.006963325276519188\n",
      "\n",
      "\n",
      "The results of jsma:\n",
      "\n",
      "tnr_at_95_tpr: 56.69867947110805\n",
      "detection_acc: 75.84933973507901\n",
      "AUROC:  97.95121743605219\n",
      "\n",
      "\n",
      "The results of cw-l2:\n",
      "\n",
      "tnr_at_95_tpr: 33.33733493357337\n",
      "detection_acc: 64.16866746631167\n",
      "AUROC:  92.80162707756119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = np.asarray(sc_id)\n",
    "ADV_DS_LIST = (\"fgsm\", \"bim-a\", \"bim-b\", \"jsma\", \"cw-l2\")\n",
    "for i, ds_name in enumerate(ADV_DS_LIST):\n",
    "    adv = id_name+\"_\"+id_model+\"_\"+ ds_name\n",
    "    das = np.load('./advs_new/'+adv+'.npy',allow_pickle=True)\n",
    "    \n",
    "    # scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "    \n",
    "    sc_ood = []\n",
    "    for i in range(len(das)):\n",
    "        x0 = base_model.predict(das[i].reshape(1,32,32,3))\n",
    "        x1 = model.predict(das[i].reshape(1,32,32,3))\n",
    "        score = mark(x0,x1)\n",
    "        sc_ood.append(-score)\n",
    "    \n",
    "    ood = np.asarray(sc_ood)\n",
    "    scores, mixed_labels = combine_inliners_outliers(test, ood)\n",
    " \n",
    "    fpr, tpr = nums(scores, mixed_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lens = id_eva_data_processed.shape[0]\n",
    " \n",
    "    FP,TN,TP,FN = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "    #     print(get_summary_statistics(scores, mixed_labels))\n",
    "    print('The results of %s:\\n' %(ds_name))\n",
    "    #     print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "    print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "    print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "    #     print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "    print(\"AUROC: \",ROC*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b5abb0188ba9781ac08b493d925deea1ef4eb1a2330813f3a395ce70cb798d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
