{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from utility import calculate_input_gradients, perturb_inputs, preprocess_images, \\\n",
    "                    postprocess_features, save_data_hdf5,get_dataset_hdf5,\\\n",
    "                    build_one_class_svm, combine_inliners_outliers, apply_temp_scale_to_model,\\\n",
    "                    apply_log_temp_scale_to_model, perturb_inputs_odin, extract_layer_features\n",
    " \n",
    "from utility_db_outliers import load_dataset\n",
    "from models_lib import load_custom_model_for_ds\n",
    "import h5py\n",
    "# from metrics2 import *\n",
    "from metrics import *\n",
    "from general_setting import *\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "#------------\n",
    "from utility_methods import *\n",
    "# from utility_methods2 import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import scipy.io as sio\n",
    "import tensorflow.keras.backend as K\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Lambda\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose, MaxPooling2D, UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name=ID_DS_LIST[0] # selects the ID dataset.\n",
    "id_model=ID_MODEL_LIST[0]  # select the deep model used for training ID dataset.\n",
    "print(id_name,id_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(org_traing_data, org_training_labels),(id_eva_data,org_testing_labels)  = load_dataset(id_name)\n",
    "\n",
    "org_traing_data_processed = preprocess_images(id_name, org_traing_data, id_model, verbose=True)\n",
    "id_eva_data_processed = preprocess_images(id_name, id_eva_data, id_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = org_traing_data_processed\n",
    "y_train = org_training_labels\n",
    "x_test = id_eva_data_processed\n",
    "y_test = org_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络参数\n",
    "input_shape = (28, 28, 1)\n",
    "batch_size = 256\n",
    "kernel_size = 3\n",
    "# structure = [16, \"average\", 32,\"average\"]\n",
    "structure = [16, 32]\n",
    "latent_dim = 784 # 隐变量取2维只是为了方便后面画图\n",
    "epochs = 50\n",
    "x_in = Input(shape=input_shape)\n",
    "x = x_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filters in structure:  \n",
    "    if isinstance(filters, int):\n",
    "        x = Conv2D(filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                strides=2,\n",
    "                padding='same')(x)\n",
    "    # elif filters == \"max\":\n",
    "    #     x = MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "    # elif filters == \"average\":\n",
    "        # x = AveragePooling2D((2, 2), padding=\"same\")(x)\n",
    "#     print(x.shape)\n",
    "x = Flatten()(x)\n",
    "# print(x.shape)\n",
    "x = Dense(latent_dim)(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Reshape((7,7,16))(x)\n",
    "for filters in structure:  \n",
    "    if isinstance(filters, int):\n",
    "        x = Conv2DTranspose(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    activation='relu',\n",
    "                    strides=2,\n",
    "                    padding='same')(x)\n",
    "        \n",
    "    # elif filters == \"max\" or filters == \"average\":\n",
    "    #     x = UpSampling2D((2, 2))(x)\n",
    "    print(x.shape)\n",
    "\n",
    "decoded = Conv2DTranspose(filters=1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        activation='sigmoid',\n",
    "                        padding='same')(x)\n",
    "print(decoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = Model(x_in, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建为一个独立的模型\n",
    "ae.compile(loss='mean_squared_error',optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.fit(x_train,x_train,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)\n",
    "# ae.fit(x_train,x_train,\n",
    "#         shuffle=True,\n",
    "#         epochs=epochs,\n",
    "#         batch_size=batch_size,\n",
    "#         validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.save('./AE_models/%s_%s_1.h5'%(id_name,id_model))\n",
    "ae.save_weights('./AE_models/%s_%s_weights_1.h5'%(id_name,id_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = ae.load_weights('./AE_models/%s_%s_weights.h5'%(id_name,id_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(x_1, x_2):\n",
    "    diff = np.abs(x_1 - x_2)\n",
    "    marks = np.mean(np.power(diff, 2), axis=(1,2,3))\n",
    "    return marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('./advs_new/%s_%s_normal.npy'%(id_name, id_model),allow_pickle=True)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_id = []\n",
    "for i in range(len(x_test)):\n",
    "    xx = ae.predict(x_test[i].reshape(1,28,28,1))\n",
    "    # print(xx.shape)   \n",
    " \n",
    "    score = mark(x_test[i],xx)\n",
    "    # print(score)\n",
    "    # break\n",
    "    sc_id.append(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inliners_outliers(inliers, outliers, i_label=1, o_label=0, verbose=False):\n",
    "\n",
    "    temp_outliers = outliers\n",
    "    temp_inliers  = inliers\n",
    "    if i_label==1:\n",
    "        i_labels = np.ones(temp_inliers.shape[0])\n",
    "    else:\n",
    "        i_labels = np.zeros(temp_inliers.shape[0])\n",
    "        \n",
    "    if o_label==0:\n",
    "        o_labels = np.zeros(temp_outliers.shape[0])\n",
    "    else:\n",
    "        o_labels = np.ones(temp_outliers.shape[0])       \n",
    "              \n",
    "    mixed_labels =  np.append(i_labels, o_labels)\n",
    "    mixed_data = np.vstack((temp_inliers, temp_outliers))\n",
    "\n",
    "    return mixed_data, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray(sc_id)\n",
    "ADV_DS_LIST = (\"fgsm\", \"bim-a\", \"bim-b\", \"jsma\", \"cw-l2\")\n",
    "for i, ds_name in enumerate(ADV_DS_LIST):\n",
    "    adv = id_name+\"_\"+id_model+\"_\"+ ds_name\n",
    "    das = np.load('./advs_new/'+adv+'.npy',allow_pickle=True)\n",
    "    sc_ood = []\n",
    "    for i in range(len(das)):\n",
    "        xx = ae.predict(das[i].reshape(1,28,28,1))\n",
    "        score = mark(das[i],xx)\n",
    "        sc_ood.append(-score)\n",
    "    \n",
    "    ood = np.asarray(sc_ood)\n",
    "    scores, mixed_labels = combine_inliners_outliers(test, ood)\n",
    " \n",
    "    fpr, tpr = nums(scores, mixed_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lens = id_eva_data_processed.shape[0]\n",
    "    FP,TN,TP,FN = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "    #     print(get_summary_statistics(scores, mixed_labels))\n",
    "    print('The results of %s:\\n' %(ds_name))\n",
    "    #     print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "    print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "    print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "    #     print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "    print(\"AUROC: \",ROC*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_inx specifies the index of the OODL found by \"find_oodl\" jupyter file.\n",
    "REM_TOP_LAYER = -2\n",
    "NUM_CLASS = 10\n",
    "if id_name==\"MNIST\" and id_model==\"LeNet\":\n",
    "    layer_inx = 0\n",
    "    OOD_DS_LIST       = OOD_DS_LIST_MNIST\n",
    "    #**********************************************************\n",
    "elif id_name==\"CIFAR10\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_CIFAR10\n",
    "    if id_model==\"VGG\":\n",
    "        layer_inx = 6\n",
    "    elif id_model==\"ResNet\":\n",
    "        layer_inx = 20\n",
    "    #**********************************************************\n",
    "elif id_name==\"SVHN\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_SVHN\n",
    "    if id_model==\"VGG\":\n",
    "        layer_inx = 6\n",
    "    elif id_model==\"ResNet\":\n",
    "        layer_inx = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vriance模块(OODL大类别 --> 删除同样的feature maps)\n",
    "test = np.asarray(sc_id)\n",
    "for i, ood_ds_name in enumerate(OOD_DS_LIST):\n",
    "    (_,_),(ood_eva_data,_) = load_dataset(ood_ds_name)\n",
    "    das = preprocess_images(id_name, ood_eva_data, id_model, verbose=True)\n",
    "    \n",
    "    # scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "    \n",
    "    sc_ood = []\n",
    "    for i in range(len(das)):\n",
    "        xx = ae.predict(das[i].reshape(1,28,28,1))\n",
    "        score = mark(das[i],xx)\n",
    "        sc_ood.append(-score)\n",
    "    \n",
    "    ood = np.asarray(sc_ood) \n",
    "    scores, mixed_labels = combine_inliners_outliers(test, ood) \n",
    "\n",
    "    fpr, tpr = nums(scores, mixed_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lens = id_eva_data_processed.shape[0]\n",
    "    FP,TN,TP,FN = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "#     print(get_summary_statistics(scores, mixed_labels))\n",
    "    print('The results of %s:\\n' %(ood_ds_name))\n",
    "#     print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "    print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "    print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "#     print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "    print(\"AUROC: \",ROC*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_ds_name = 'foolingimages'\n",
    "\n",
    "das = np.load('adv_datasets/%s_%s_fooling_images.npy'%(id_name,id_model))\n",
    "\n",
    "# scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "# das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "# das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "\n",
    "sc_ood = []\n",
    "for i in range(len(das)):\n",
    "        xx = ae.predict(das[i].reshape(1,28,28,1))\n",
    "        score = mark(das[i],xx)\n",
    "        sc_ood.append(-score)\n",
    "\n",
    "ood = np.asarray(sc_ood) \n",
    "scores, mixed_labels = combine_inliners_outliers(test, ood) \n",
    "\n",
    "# print('scores is:', (scores[10000:20000]))\n",
    "fpr, tpr = nums(scores, mixed_labels)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "lens = id_eva_data_processed.shape[0]\n",
    "FP,TN,TP,FN = ErrorRateAt95Recall1(lens,scores, mixed_labels)\n",
    "ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "print('The results of %s:\\n' %(ood_ds_name))\n",
    "# print(get_summary_statistics(scores, mixed_labels))\n",
    "# print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "# print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "print(\"AUROC: \",ROC*100)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b5abb0188ba9781ac08b493d925deea1ef4eb1a2330813f3a395ce70cb798d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
