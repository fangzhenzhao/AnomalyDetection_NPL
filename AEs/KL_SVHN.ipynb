{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from utility import calculate_input_gradients, perturb_inputs, preprocess_images, \\\n",
    "                    postprocess_features, save_data_hdf5,get_dataset_hdf5,\\\n",
    "                    build_one_class_svm, combine_inliners_outliers, apply_temp_scale_to_model,\\\n",
    "                    apply_log_temp_scale_to_model, perturb_inputs_odin, extract_layer_features\n",
    " \n",
    "from utility_db_outliers import load_dataset\n",
    "from models_lib import load_custom_model_for_ds\n",
    "import h5py\n",
    "# from metrics2 import *\n",
    "from metrics import *\n",
    "from general_setting import *\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "#------------\n",
    "from utility_methods import *\n",
    "# from utility_methods2 import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import scipy.io as sio\n",
    "import tensorflow.keras.backend as K\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Lambda\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose, MaxPooling2D, UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVHN ResNet\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "SAVE_RESULTS = True\n",
    "id_name=ID_DS_LIST[2] # selects the ID dataset.\n",
    "id_model=ID_MODEL_LIST[2]  # select the deep model used for training ID dataset.\n",
    "print(id_name,id_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ResNet-V1-20 model for SVHN was loaded.\n"
     ]
    }
   ],
   "source": [
    "org_model = load_custom_model_for_ds(id_name, id_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Conv2D, Conv2DTranspose, Dense, Flatten,\n",
    "                                     InputLayer, Reshape, Layer, Input)\n",
    "from typing import Callable, Dict, List, Tuple, Union\n",
    "from tensorflow.keras.losses import kld, categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = org_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu',strides=2, padding='same', input_shape=(32, 32, 3),name='conv1'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu',strides=2, padding='same',name='conv2'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu',strides=2, padding='same',name='conv3'))\n",
    "    model.add(Conv2DTranspose(64, (3, 3), activation='relu',strides=2, padding='same'))\n",
    "    model.add(Conv2DTranspose(32, (3, 3), activation='relu',strides=2, padding='same'))\n",
    "    model.add(Conv2DTranspose(3, (3, 3), activation='sigmoid',strides=2, padding='same'))\n",
    "    \n",
    "    # model.add(Conv2DTranspose(3, (3, 3), activation=None,strides=2, padding='same'))\n",
    "\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_model = head_model()\n",
    "head_model.add(base_model)\n",
    "model = head_model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing was done for  SVHN ResNet\n",
      "Preprocessing was done for  SVHN ResNet\n"
     ]
    }
   ],
   "source": [
    "(org_traing_data, org_training_labels),(id_eva_data,org_testing_labels)  = load_dataset(id_name)\n",
    "\n",
    "org_traing_data_processed = preprocess_images(id_name, org_traing_data, id_model, verbose=True)\n",
    "id_eva_data_processed = preprocess_images(id_name, id_eva_data, id_model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = org_traing_data_processed\n",
    "y_train = org_training_labels\n",
    "x_test = id_eva_data_processed\n",
    "y_test = org_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler( )\n",
    "scaler.fit(x_test.flatten().reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "x_test_0 = scaler.transform(x_test.flatten().reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "x_test = x_test_0.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],x_test.shape[3])\n",
    "\n",
    "scaler.fit(x_train.flatten().reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]*x_train.shape[3]))\n",
    "x_train_0 = scaler.transform(x_train.flatten().reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]*x_train.shape[3]))\n",
    "x_train = x_train_0.reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2],x_train.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = base_model.predict(x_train)\n",
    "print(y_true.shape)\n",
    "np.save('./KL_datas/%s_%s_train.npy'%(id_name, id_model), x_train)\n",
    "np.save('./KL_datas/%s_%s_preds.npy'%(id_name, id_model), y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train= np.load('./KL_datas/%s_%s_train.npy'%(id_name, id_model))\n",
    "y_true = np.load('./KL_datas/%s_%s_preds.npy'%(id_name, id_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814/814 [==============================] - 7s 9ms/step - loss: 0.2379 - accuracy: 0.9612\n",
      "[0.23785312473773956, 0.9612016081809998]\n"
     ]
    }
   ],
   "source": [
    "y_test = to_categorical(y_test,10)\n",
    "sc = base_model.evaluate(x_test,y_test)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if str(layer.name) == 'model':\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss_fn(y_true, y_pred):\n",
    "    temperature = 150\n",
    "    y_true = y_true ** (1 / temperature)\n",
    "    loss_kld = kld(y_true, y_pred)\n",
    "    # std_kld = tf.math.reduce_std(loss_kld)\n",
    "    loss = tf.reduce_mean(loss_kld)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=my_loss_fn, optimizer=Adam(lr=1e-2), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=256, epochs=30)\n",
    "model.save('./KL_models/%s_%s_KL.h5'%(id_name, id_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 16, 16, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 8, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 64)          73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 3)         867       \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 10)                274442    \n",
      "=================================================================\n",
      "Total params: 460,813\n",
      "Trainable params: 186,371\n",
      "Non-trainable params: 274,442\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./KL_models/%s_%s_KL.h5'%(id_name,id_model), custom_objects={'my_loss_fn': my_loss_fn})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = base_model.predict(x_test[0].reshape(1,32,32,3))\n",
    "x1 = model.predict(x_test[0].reshape(1,32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = Model(inputs=model.input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = feature_model.predict(x_test[0].reshape(1,32,32,3))\n",
    "print(img1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img1.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_loss_fn(x0,x1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "das = np.load('./advs_new/%s_%s_fgsm.npy'%(id_name, id_model),allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(22.231861, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y0 = base_model.predict(das[0].reshape(1,32,32,3))\n",
    "y1 = model.predict(das[0].reshape(1,32,32,3))\n",
    "b = my_loss_fn(y0,y1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(21.144073, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y0 = base_model.predict(x_test[0].reshape(1,32,32,3))\n",
    "y1 = model.predict(x_test[0].reshape(1,32,32,3))\n",
    "b = my_loss_fn(y0,y1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(y_true, y_pred):\n",
    "    temperature = 150\n",
    "    y_true = y_true ** (1 / temperature)\n",
    "    loss_kld = kld(y_true, y_pred)\n",
    "    # std_kld = tf.math.reduce_std(loss_kld)\n",
    "    loss = tf.reduce_mean(loss_kld)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_id = []\n",
    "for i in range(len(x_test)):\n",
    "    x0 = base_model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    x1 = model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    score = mark(x0,x1)\n",
    "    # print(score)\n",
    "    sc_id.append(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inliners_outliers(inliers, outliers, i_label=1, o_label=0, verbose=False):\n",
    "\n",
    "    temp_outliers = outliers\n",
    "    temp_inliers  = inliers\n",
    "    if i_label==1:\n",
    "        i_labels = np.ones(temp_inliers.shape[0])\n",
    "    else:\n",
    "        i_labels = np.zeros(temp_inliers.shape[0])\n",
    "        \n",
    "    if o_label==0:\n",
    "        o_labels = np.zeros(temp_outliers.shape[0])\n",
    "    else:\n",
    "        o_labels = np.ones(temp_outliers.shape[0])       \n",
    "              \n",
    "    mixed_labels =  np.append(i_labels, o_labels)\n",
    "    mixed_data = np.hstack((temp_inliers, temp_outliers))\n",
    "\n",
    "    return mixed_data, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_inx specifies the index of the OODL found by \"find_oodl\" jupyter file.\n",
    "REM_TOP_LAYER = -2\n",
    "NUM_CLASS = 10\n",
    "if id_name==\"MNIST\" and id_model==\"LeNet\":\n",
    "    layer_inx = 0\n",
    "    OOD_DS_LIST       = OOD_DS_LIST_MNIST\n",
    "    #**********************************************************\n",
    "elif id_name==\"CIFAR10\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_CIFAR10\n",
    "    if id_model==\"VGG\":\n",
    "        layer_inx = 6\n",
    "    elif id_model==\"ResNet\":\n",
    "        layer_inx = 20\n",
    "    #**********************************************************\n",
    "elif id_name==\"SVHN\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_SVHN\n",
    "    if id_model==\"VGG\":\n",
    "        layer_inx = 6\n",
    "    elif id_model==\"ResNet\":\n",
    "        layer_inx = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing was done for  SVHN ResNet\n",
      "The results of TINYIMAGENET_RESIZED_32:\n",
      "\n",
      "tnr_at_95_tpr: 85.77999999914219\n",
      "detection_acc: 90.38923171420117\n",
      "AUROC:  97.21434811001845\n",
      "\n",
      "\n",
      "Preprocessing was done for  SVHN ResNet\n",
      "The results of LSUN_RESIZED:\n",
      "\n",
      "tnr_at_95_tpr: 83.45999999916539\n",
      "detection_acc: 89.22923171421277\n",
      "AUROC:  96.76434042716654\n",
      "\n",
      "\n",
      "Preprocessing was done for  SVHN ResNet\n",
      "The results of ISUN_PATCHED:\n",
      "\n",
      "tnr_at_95_tpr: 86.75630252003633\n",
      "detection_acc: 90.87738297464824\n",
      "AUROC:  97.36419214274524\n",
      "\n",
      "\n",
      "Preprocessing was done for  SVHN ResNet\n",
      "The results of CIFAR10:\n",
      "\n",
      "tnr_at_95_tpr: 85.62999999914369\n",
      "detection_acc: 90.31423171420192\n",
      "AUROC:  97.13112611401353\n",
      "\n",
      "\n",
      "Preprocessing was done for  SVHN ResNet\n",
      "The results of G_255:\n",
      "\n",
      "tnr_at_95_tpr: 93.6099999990639\n",
      "detection_acc: 94.30423171416203\n",
      "AUROC:  98.36711950676091\n",
      "\n",
      "\n",
      "Preprocessing was done for  SVHN ResNet\n",
      "The results of U_255:\n",
      "\n",
      "tnr_at_95_tpr: 93.43999999906559\n",
      "detection_acc: 94.21923171416286\n",
      "AUROC:  98.36677742778119\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vriance模块(OODL大类别 --> 删除同样的feature maps)\n",
    "test = np.asarray(sc_id)\n",
    "for i, ood_ds_name in enumerate(OOD_DS_LIST):\n",
    "    (_,_),(ood_eva_data,_) = load_dataset(ood_ds_name)\n",
    "    das = preprocess_images(id_name, ood_eva_data, id_model, verbose=True)\n",
    "    \n",
    "    # scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "    # das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "    \n",
    "    sc_ood = []\n",
    "    for i in range(len(das)):\n",
    "        x0 = base_model.predict(das[i].reshape(1,32,32,3))\n",
    "        x1 = model.predict(das[i].reshape(1,32,32,3))\n",
    "        score = mark(x0,x1)\n",
    "        sc_ood.append(-score)\n",
    "    \n",
    "    ood = np.asarray(sc_ood) \n",
    "    scores, mixed_labels = combine_inliners_outliers(test, ood) \n",
    "\n",
    "    fpr, tpr = nums(scores, mixed_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lens = id_eva_data_processed.shape[0]\n",
    "    FP,TN,TP,FN = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "#     print(get_summary_statistics(scores, mixed_labels))\n",
    "    print('The results of %s:\\n' %(ood_ds_name))\n",
    "#     print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "    print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "    print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "#     print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "    print(\"AUROC: \",ROC*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of foolingimages:\n",
      "\n",
      "tnr_at_95_tpr: 0.0\n",
      "detection_acc: 47.49923171463008\n",
      "AUROC:  46.7566606484327\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ood_ds_name = 'foolingimages'\n",
    "\n",
    "das = np.load('adv_datasets/%s_%s_fooling_images.npy'%(id_name,id_model))\n",
    "\n",
    "# scaler.fit(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "# das_0 = scaler.transform(das.flatten().reshape(das.shape[0],das.shape[1]*das.shape[2]*das.shape[3]))\n",
    "# das = das_0.reshape(das.shape[0],das.shape[1],das.shape[2],das.shape[3])\n",
    "\n",
    "sc_ood = []\n",
    "for i in range(len(das)):\n",
    "        x0 = base_model.predict(das[i].reshape(1,32,32,3))\n",
    "        x1 = model.predict(das[i].reshape(1,32,32,3))\n",
    "        score = mark(x0,x1)\n",
    "        sc_ood.append(-score)\n",
    "\n",
    "ood = np.asarray(sc_ood) \n",
    "scores, mixed_labels = combine_inliners_outliers(test, ood) \n",
    "\n",
    "# print('scores is:', (scores[10000:20000]))\n",
    "fpr, tpr = nums(scores, mixed_labels)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "lens = id_eva_data_processed.shape[0]\n",
    "FP,TN,TP,FN = ErrorRateAt95Recall1(lens,scores, mixed_labels)\n",
    "ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "print('The results of %s:\\n' %(ood_ds_name))\n",
    "# print(get_summary_statistics(scores, mixed_labels))\n",
    "# print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "# print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "print(\"AUROC: \",ROC*100)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25022, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_test = np.load('./advs_new/%s_%s_normal.npy'%(id_name, id_model),allow_pickle=True)\n",
    "# scaler.fit(x_test.flatten().reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "# x_test_0 = scaler.transform(x_test.flatten().reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]*x_test.shape[3]))\n",
    "# x_test = x_test_0.reshape(x_test.shape[0],x_test.shape[1],x_test.shape[2],x_test.shape[3])\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_id = []\n",
    "for i in range(len(x_test)):\n",
    "    x0 = base_model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    x1 = model.predict(x_test[i].reshape(1,32,32,3))\n",
    "    score = mark(x0,x1)\n",
    "    # print(score)\n",
    "    sc_id.append(-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./KL_datas/%s_%s_normal.npy'%(id_name, id_model),sc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_id = np.load('./KL_datas/%s_%s_normal.npy'%(id_name, id_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of fgsm:\n",
      "\n",
      "tnr_at_95_tpr: 74.89172080595165\n",
      "detection_acc: 84.9450921176059\n",
      "AUROC:  97.61564463189266\n",
      "\n",
      "\n",
      "The results of bim-a:\n",
      "\n",
      "tnr_at_95_tpr: 53.081792436893714\n",
      "detection_acc: 74.04012793307693\n",
      "AUROC:  89.40303478506813\n",
      "\n",
      "\n",
      "The results of bim-b:\n",
      "\n",
      "tnr_at_95_tpr: 0.0\n",
      "detection_acc: 47.49923171463008\n",
      "AUROC:  0.0009664582849931165\n",
      "\n",
      "\n",
      "The results of jsma:\n",
      "\n",
      "tnr_at_95_tpr: 70.11910711281809\n",
      "detection_acc: 82.55878527103913\n",
      "AUROC:  97.0351600942062\n",
      "\n",
      "\n",
      "The results of cw-l2:\n",
      "\n",
      "tnr_at_95_tpr: 38.30584707630224\n",
      "detection_acc: 66.6521552527812\n",
      "AUROC:  91.81862323875795\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = np.asarray(sc_id)\n",
    "ADV_DS_LIST = (\"fgsm\", \"bim-a\", \"bim-b\", \"jsma\", \"cw-l2\")\n",
    "for i, ds_name in enumerate(ADV_DS_LIST):\n",
    "    adv = id_name+\"_\"+id_model+\"_\"+ ds_name\n",
    "    das = np.load('./advs_new/'+adv+'.npy',allow_pickle=True)\n",
    "    \n",
    "    sc_ood = []\n",
    "    for i in range(len(das)):\n",
    "        x0 = base_model.predict(das[i].reshape(1,32,32,3))\n",
    "        x1 = model.predict(das[i].reshape(1,32,32,3))\n",
    "        score = mark(x0,x1)\n",
    "        sc_ood.append(-score)\n",
    "    \n",
    "    ood = np.asarray(sc_ood)\n",
    "    scores, mixed_labels = combine_inliners_outliers(test, ood)\n",
    " \n",
    "    fpr, tpr = nums(scores, mixed_labels)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    lens = id_eva_data_processed.shape[0]\n",
    "    FP,TN,TP,FN = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "    #     print(get_summary_statistics(scores, mixed_labels))\n",
    "    print('The results of %s:\\n' %(ds_name))\n",
    "    #     print('fpr_at_95_tpr:', float(FP) / float(FP + TN+ 1e-7) *100)\n",
    "    print('tnr_at_95_tpr:', float(TN) / float(FP + TN+ 1e-7) *100)\n",
    "    print('detection_acc:',(float(TP) / float(TP + FN + 1e-7)+ float(TN) / float(FP + TN + 1e-7))/2*100)\n",
    "    #     print('detection_errror:',(1.0- float(TP) / float(TP + FN+ 1e-7)+ float(FP) / float(FP + TN+ 1e-7))/2*100)\n",
    "    print(\"AUROC: \",ROC*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31b5abb0188ba9781ac08b493d925deea1ef4eb1a2330813f3a395ce70cb798d"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
