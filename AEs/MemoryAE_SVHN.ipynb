{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from utility import calculate_input_gradients, perturb_inputs, preprocess_images, \\\n",
    "                    postprocess_features, save_data_hdf5,get_dataset_hdf5,\\\n",
    "                    build_one_class_svm, combine_inliners_outliers, apply_temp_scale_to_model,\\\n",
    "                    apply_log_temp_scale_to_model, perturb_inputs_odin, extract_layer_features\n",
    " \n",
    "from utility_db_outliers import load_dataset\n",
    "from models_lib import load_custom_model_for_ds\n",
    "import h5py\n",
    "# from metrics2 import *\n",
    "from metrics import *\n",
    "from general_setting import *\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "#------------\n",
    "from utility_methods import *\n",
    "# from utility_methods2 import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import gzip\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import scipy.io as sio\n",
    "import tensorflow.keras.backend as K\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_name=ID_DS_LIST[2] # selects the ID dataset.\n",
    "id_model=ID_MODEL_LIST[2]  # select the deep model used for training ID dataset.\n",
    "print(id_name,id_model)\n",
    "\n",
    "(org_traing_data, org_training_labels),(id_eva_data,org_testing_labels)  = load_dataset(id_name)\n",
    "org_traing_data_processed = preprocess_images(id_name, org_traing_data, id_model, verbose=True)\n",
    "id_eva_data_processed = preprocess_images(id_name, id_eva_data, id_model, verbose=True)\n",
    "\n",
    "x_train = org_traing_data_processed\n",
    "y_train = org_training_labels\n",
    "x_test = id_eva_data_processed\n",
    "y_test = org_testing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_class = 5\n",
    "\n",
    "x_train_normal = x_train[y_train ==normal_class]\n",
    "y_train_normal = y_train[y_train==normal_class]\n",
    "\n",
    "x_train_anomaly = x_train[y_train!=normal_class]\n",
    "y_train_anomaly= y_train[y_train!=normal_class]\n",
    "\n",
    "x_test_normal = x_test[y_test==normal_class]\n",
    "y_test_normal = y_test[y_test==normal_class]\n",
    "\n",
    "x_test_anomaly = x_test[y_test!=normal_class]\n",
    "y_test_anomaly = y_test[y_test!=normal_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank(layers.Layer):\n",
    "    def __init__(self, mem_dim, fea_dim):\n",
    "        super(MemoryBank, self).__init__()\n",
    "        self.mem_dim = mem_dim\n",
    "        self.fea_dim = fea_dim\n",
    "\n",
    "        memory_weight_init = tf.random_normal_initializer()\n",
    "        self.memory_weight = tf.Variable(\n",
    "            initial_value=memory_weight_init(shape=(self.mem_dim, self.fea_dim), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "    def cosine_sim(self, a, b):\n",
    "        num = tf.linalg.matmul(a, tf.transpose(b, perm=[1, 0]))\n",
    "        denom_a =  tf.math.sqrt(tf.math.reduce_sum(a**2,axis=1,keepdims=True))\n",
    "        denom_b = tf.math.sqrt(tf.math.reduce_sum(b**2,axis=1,keepdims=True))\n",
    "        denom =  tf.linalg.matmul(denom_a,tf.transpose(denom_b))\n",
    "        w = (num + 1e-12) / (denom + 1e-12)\n",
    "        return w   \n",
    "\n",
    "    def sparse_shink(self,x,mem_dim):\n",
    "        # lamb = 1 / mem_dim # may need to be tuned!!!\n",
    "        lamb = 0.0025\n",
    "        num = tf.keras.activations.relu(x - lamb)*x\n",
    "        denom = tf.abs(x - lamb) + 1e-12\n",
    "        wh = (num + 1e-12) / denom \n",
    "        wh_renorm = wh/tf.norm(wh,ord=1,axis=1,keepdims=True)\n",
    "        return wh_renorm \n",
    "        \n",
    "    def call(self, z):\n",
    "        d = self.cosine_sim(z,self.memory_weight)\n",
    "        w = tf.nn.softmax(d)\n",
    "        wh_renorm = self.sparse_shink(w,self.mem_dim)\n",
    "        z_hat = tf.linalg.matmul(wh_renorm,self.memory_weight)\n",
    "        return z_hat, wh_renorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemAE(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MemAE, self).__init__(**kwargs)\n",
    "        self.encoder = self.getEncoder()\n",
    "        self.decoder = self.getDecoder()\n",
    "        \n",
    "    def getEncoder(self):\n",
    "        encoder_inputs = keras.Input(shape=(32, 32, 3))\n",
    "        # x = keras.layers.Flatten()(encoder_inputs)\n",
    "        x = layers.Conv2D(32, 3, strides=2, padding='same', activation=tf.nn.relu)(encoder_inputs)\n",
    "        x = layers.Conv2D(64, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "        x = layers.Conv2D(128, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        z_hat, wh_renorm = MemoryBank(800,2048)(x)\n",
    "        encoder = keras.Model(encoder_inputs, [z_hat, wh_renorm], name=\"encoder\")\n",
    "        return encoder \n",
    "        \n",
    "    def getDecoder(self):\n",
    "        latent_inputs = keras.Input(shape=(2048,))\n",
    "        x = layers.Reshape(target_shape=(4, 4, 128))(latent_inputs)\n",
    "        \n",
    "        x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "        x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation=tf.nn.relu)(x)\n",
    "        x = layers.Conv2DTranspose(3, 3, strides=2, padding='same', activation='sigmoid')(x)\n",
    "        decoder_outputs = keras.layers.Reshape(target_shape=(32, 32, 3))(x)\n",
    "        decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "        return decoder\n",
    "        \n",
    "    def train_step(self, x):\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            z_hat, wh_renorm = self.encoder(x)\n",
    "            x_hat = self.decoder(z_hat)\n",
    "\n",
    "            mse = tf.reduce_sum(tf.square(x - x_hat))\n",
    "            mem_etrp = tf.reduce_sum((-wh_renorm) * tf.math.log(wh_renorm + 1e-12))\n",
    "            loss = tf.reduce_mean(mse + (0.0002 * mem_etrp))\n",
    "            \n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data[0],data[1]\n",
    "        z_hat, wh_renorm = self.encoder(x)\n",
    "        x_hat = self.decoder(z_hat)\n",
    "\n",
    "        mse = tf.reduce_sum(tf.square(x - x_hat))\n",
    "        mem_etrp = tf.reduce_sum((-wh_renorm) * tf.math.log(wh_renorm + 1e-12))\n",
    "        loss = tf.reduce_mean(mse + (0.0002 * mem_etrp))\n",
    "        return {\"loss\": loss}\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_hat, w_hat = self.encoder(inputs)\n",
    "        x_hat = self.decoder(z_hat)\n",
    "        return x_hat, w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model = MemAE()\n",
    "    #model.compile(optimizer=keras.optimizers.Adam(),run_eagerly=True)\n",
    "    model.compile(optimizer=keras.optimizers.Adam())\n",
    "    # model.compile(loss='mean_squared_error',optimizer='rmsprop')\n",
    "    normal_class = i\n",
    "    x_train_normal = x_train[y_train ==normal_class]\n",
    "    x_test_normal = x_test[y_test==normal_class]\n",
    "    model.fit(x_train_normal,validation_data=(x_test_normal,x_test_normal), epochs=50, batch_size=256)\n",
    "    \n",
    "    # model.save('./MemAE_models/%s_%s_%s.h5'%(id_name,id_model,i))\n",
    "    model.save_weights('./MemAE_models/%s_%s_weights_%s.h5'%(id_name,id_model,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     a,_ = model(x_test_anomaly[i].reshape(1,28,28,1))\n",
    "#     plt.imshow(x_test_anomaly[i,:,:],'gray')\n",
    "#     plt.show() \n",
    "#     plt.imshow(a[0,:,:],'gray')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(x_1, x_2):\n",
    "    diff = np.abs(x_1 - x_2)\n",
    "    marks = np.mean(np.power(diff, 2), axis=(1,2,3))\n",
    "    return marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inliners_outliers(inliers, outliers, i_label=1, o_label=0, verbose=False):\n",
    "\n",
    "    temp_outliers = outliers\n",
    "    temp_inliers  = inliers\n",
    "    if i_label==1:\n",
    "        i_labels = np.ones(temp_inliers.shape[0])\n",
    "    else:\n",
    "        i_labels = np.zeros(temp_inliers.shape[0])\n",
    "        \n",
    "    if o_label==0:\n",
    "        o_labels = np.zeros(temp_outliers.shape[0])\n",
    "    else:\n",
    "        o_labels = np.ones(temp_outliers.shape[0])       \n",
    "              \n",
    "    mixed_labels =  np.append(i_labels, o_labels)\n",
    "    mixed_data = np.vstack((temp_inliers, temp_outliers))\n",
    "\n",
    "    return mixed_data, mixed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimg = np.load('./datas/testimgs_%s_%s.npy'%(id_model,id_name),allow_pickle=True)\n",
    "sc_id = [[] for i in range(10)]\n",
    "for c in range(10):\n",
    "    normal_class = c \n",
    "    \n",
    "    new_model = MemAE()\n",
    "    x, _ = new_model(x_test[0].reshape(1,32,32,3))\n",
    "    new_model.load_weights('./MemAE_models/%s_%s_weights_%s.h5'%(id_name,id_model,normal_class))\n",
    "    model = new_model \n",
    "    \n",
    "    x_test_normal = testimg[normal_class]\n",
    "    for i in range(len(x_test_normal)):\n",
    "        xx,_ = model(x_test_normal[i].reshape(1,32,32,3))\n",
    "        score = mark(x_test_normal[i],xx)\n",
    "        sc_id[c].append(-score)\n",
    "        \n",
    "np.save('./MemAE_datas_OOD/testimgs_%s_%s.npy'%(id_model,id_name),sc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer_inx specifies the index of the OODL found by \"find_oodl\" jupyter file.\n",
    "REM_TOP_LAYER = -2\n",
    "NUM_CLASS = 10\n",
    "if id_name==\"MNIST\" and id_model==\"LeNet\":\n",
    "    OOD_DS_LIST       = OOD_DS_LIST_MNIST\n",
    "    #**********************************************************\n",
    "elif id_name==\"CIFAR10\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_CIFAR10\n",
    "    #**********************************************************\n",
    "elif id_name==\"SVHN\":\n",
    "    OOD_DS_LIST      = OOD_DS_LIST_SVHN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimg = np.load('./datas/testimgs_%s_%s.npy'%(id_model,id_name),allow_pickle=True)\n",
    "sc_id = np.load('./MemAE_datas_OOD/testimgs_%s_%s.npy'%(id_model,id_name),allow_pickle=True)\n",
    "\n",
    "for i, ds_name in enumerate(OOD_DS_LIST):\n",
    "    ood = id_name+\"_\"+id_model+\"_\"+ ds_name\n",
    "    das = np.load('ClassesDatas/'+ood+'.npy',allow_pickle=True)\n",
    "    lists = []\n",
    "    for i in range(10):\n",
    "        if (len(das[i]) != 0):\n",
    "            lists.append(i)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    FP = dict()\n",
    "    TN = dict()\n",
    "    TP = dict()\n",
    "    FN = dict()\n",
    "    temp = []\n",
    "    la = []\n",
    "    ROC = 0\n",
    "    numclass = len(das)\n",
    "    for c in lists:\n",
    "        normal_class = c \n",
    "        \n",
    "        new_model = MemAE()\n",
    "        x, _ = new_model(x_test[0].reshape(1,32,32,3))\n",
    "        new_model.load_weights('./MemAE_models/%s_%s_weights_%s.h5'%(id_name,id_model,normal_class))\n",
    "        model = new_model \n",
    "       \n",
    "            \n",
    "        sc_ood = []\n",
    "        for i in range(len(das[normal_class])):\n",
    "            xx,_ = model(das[normal_class][i].reshape(1,32,32,3))\n",
    "            score = mark(das[normal_class][i],xx)\n",
    "            sc_ood.append(-score)\n",
    "        \n",
    "        test = np.asarray(sc_id[normal_class])\n",
    "        ood = np.asarray(sc_ood)\n",
    "    \n",
    "        scores, mixed_labels = combine_inliners_outliers(test, ood)\n",
    "\n",
    "        # fpr[c], tpr[c] = nums(scores, mixed_labels)\n",
    "        # roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "        # print(\"%d th AUROC: %f\"%(c,roc_auc[c]))\n",
    "        \n",
    "        lens = test.shape[0]\n",
    "        FP[c],TN[c],TP[c],FN[c] = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    #     print(FP[i],TN[i],TP[i],FN[i])\n",
    "        ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "        # print(\"%d th AUROC: %f\"%(c,ROC))   \n",
    "        temp.append(ROC)\n",
    "        la.append(mixed_labels)\n",
    "    for i in range(len(lists)):\n",
    "        ROC += float(temp[i])       \n",
    "    \n",
    "    numbers_test = 0\n",
    "    number_ood = 0\n",
    "    for i in lists:\n",
    "        numbers_test+= len(testimg[i])\n",
    "        number_ood += len(das[i])\n",
    "    numbers_all =  numbers_test +  number_ood\n",
    "#     print('numbers_all is:',numbers_all)\n",
    "\n",
    "    ROC_AVE = 0\n",
    "    for i in range(len(lists)):\n",
    "        ROC_AVE += float(len(la[i]/numbers_all)*temp[i])\n",
    "    ROC_AVE = ROC_AVE/numbers_all\n",
    "\n",
    "    NFP = 0\n",
    "    NTN = 0\n",
    "    NTP = 0\n",
    "    NFN = 0\n",
    "    for i in lists:\n",
    "        NFP += FP[i]\n",
    "    for i in lists:\n",
    "        NTN += TN[i]\n",
    "    for i in lists:\n",
    "        NTP += TP[i]\n",
    "    for i in lists:\n",
    "        NFN += FN[i]\n",
    "    print('The results of %s:\\n'%(ds_name))\n",
    "    #     print('fpr_at_95_tpr:', float(NFP) / float(NFP + NTN+ 1e-7) *100)\n",
    "    #     print('detection_errror:',(1.0- float(NTP) / float(NTP + NFN + 1e-7)+ float(NFP) / float(NFP + NTN + 1e-7))/2*100)\n",
    "    print('tnr_at_95_tpr:', float(NTN) / float(NFP + NTN+ 1e-7) *100)\n",
    "\n",
    "    print('detection_acc:',(float(NTP) / float(NTP + NFN + 1e-7)+ float(NTN) / float(NFP + NTN + 1e-7))/2*100)\n",
    "#     print(\"AUROC: \",ROC/(len(lists)+1)*100)\n",
    "    print(\"Average AUROC: \",ROC_AVE*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Adversarial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimg = np.load('datas_new/testimgs_%s_%s.npy'%(id_model,id_name),allow_pickle=True)\n",
    "sc_id = [[] for i in range(10)]\n",
    "for c in range(10):\n",
    "    normal_class = c \n",
    "    \n",
    "    new_model = MemAE()\n",
    "    x, _ = new_model(x_test[0].reshape(1,32,32,3))\n",
    "    new_model.load_weights('./MemAE_models/%s_%s_weights_%s.h5'%(id_name,id_model,normal_class))\n",
    "    model = new_model \n",
    "    \n",
    "    x_test_normal = testimg[normal_class]\n",
    "    for i in range(len(x_test_normal)):\n",
    "        xx,_ = model(x_test_normal[i].reshape(1,32,32,3))\n",
    "        score = mark(x_test_normal[i],xx)\n",
    "        sc_id[c].append(-score)\n",
    "        \n",
    "np.save('./MemAE_datas_AD/testimgs_%s_%s.npy'%(id_model,id_name),sc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimg = np.load('./datas_new/testimgs_%s_%s.npy'%(id_model,id_name),allow_pickle=True)\n",
    "sc_id = np.load('./MemAE_datas_AD/testimgs_%s_%s.npy'%(id_model,id_name),allow_pickle=True)\n",
    "ADV_DS_LIST = (\"fgsm\", \"bim-a\", \"bim-b\", \"jsma\", \"cw-l2\",'fooling_images')\n",
    "\n",
    "for i, ds_name in enumerate(ADV_DS_LIST):\n",
    "    ood = id_name+\"_\"+id_model+\"_\"+ ds_name\n",
    "    das = np.load('classdatas_new/'+ood+'.npy',allow_pickle=True)\n",
    "    lists = []\n",
    "    for i in range(10):\n",
    "        if (len(das[i]) != 0):\n",
    "            lists.append(i)\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    FP = dict()\n",
    "    TN = dict()\n",
    "    TP = dict()\n",
    "    FN = dict()\n",
    "    temp = []\n",
    "    la = []\n",
    "    ROC = 0\n",
    "    numclass = len(das)\n",
    "    for c in lists:\n",
    "        normal_class = c \n",
    "        \n",
    "        new_model = MemAE()\n",
    "        x, _ = new_model(x_test[0].reshape(1,32,32,3))\n",
    "        new_model.load_weights('./MemAE_models/%s_%s_weights_%s.h5'%(id_name,id_model,normal_class))\n",
    "        model = new_model \n",
    "        \n",
    "        # sc_id = []\n",
    "        # x_test_normal = testimg[normal_class]\n",
    "        # for i in range(len(x_test_normal)):\n",
    "        #     xx,_ = model(x_test_normal[i].reshape(1,28,28,1))\n",
    "        #     score = mark(x_test_normal[i],xx)\n",
    "        #     sc_id.append(-score)\n",
    "        \n",
    "            \n",
    "        sc_ood = []\n",
    "        for i in range(len(das[normal_class])):\n",
    "            xx,_ = model(das[normal_class][i].reshape(1,32,32,3))\n",
    "            score = mark(das[normal_class][i],xx)\n",
    "            sc_ood.append(-score)\n",
    "        \n",
    "        test = np.asarray(sc_id[normal_class])\n",
    "        ood = np.asarray(sc_ood)\n",
    "    \n",
    "        scores, mixed_labels = combine_inliners_outliers(test, ood)\n",
    "\n",
    "        # fpr[c], tpr[c] = nums(scores, mixed_labels)\n",
    "        # roc_auc[c] = auc(fpr[c], tpr[c])\n",
    "        # print(\"%d th AUROC: %f\"%(c,roc_auc[c]))\n",
    "        \n",
    "        lens = test.shape[0]\n",
    "        FP[c],TN[c],TP[c],FN[c] = ErrorRateAt95Recall1(lens, scores, mixed_labels)\n",
    "    #     print(FP[i],TN[i],TP[i],FN[i])\n",
    "        ROC = roc_auc_score(mixed_labels, scores, average='micro', sample_weight=None)\n",
    "        # print(\"%d th AUROC: %f\"%(c,ROC))   \n",
    "        temp.append(ROC)\n",
    "        la.append(mixed_labels)\n",
    "    for i in range(len(lists)):\n",
    "        ROC += float(temp[i])       \n",
    "    \n",
    "    numbers_test = 0\n",
    "    number_ood = 0\n",
    "    for i in lists:\n",
    "        numbers_test+= len(testimg[i])\n",
    "        number_ood += len(das[i])\n",
    "    numbers_all =  numbers_test +  number_ood\n",
    "#     print('numbers_all is:',numbers_all)\n",
    "\n",
    "    ROC_AVE = 0\n",
    "    for i in range(len(lists)):\n",
    "        ROC_AVE += float(len(la[i]/numbers_all)*temp[i])\n",
    "    ROC_AVE = ROC_AVE/numbers_all\n",
    "\n",
    "    NFP = 0\n",
    "    NTN = 0\n",
    "    NTP = 0\n",
    "    NFN = 0\n",
    "    for i in lists:\n",
    "        NFP += FP[i]\n",
    "    for i in lists:\n",
    "        NTN += TN[i]\n",
    "    for i in lists:\n",
    "        NTP += TP[i]\n",
    "    for i in lists:\n",
    "        NFN += FN[i]\n",
    "    print('The results of %s:\\n'%(ds_name))\n",
    "    #     print('fpr_at_95_tpr:', float(NFP) / float(NFP + NTN+ 1e-7) *100)\n",
    "    #     print('detection_errror:',(1.0- float(NTP) / float(NTP + NFN + 1e-7)+ float(NFP) / float(NFP + NTN + 1e-7))/2*100)\n",
    "    print('tnr_at_95_tpr:', float(NTN) / float(NFP + NTN+ 1e-7) *100)\n",
    "\n",
    "    print('detection_acc:',(float(NTP) / float(NTP + NFN + 1e-7)+ float(NTN) / float(NFP + NTN + 1e-7))/2*100)\n",
    "#     print(\"AUROC: \",ROC/(len(lists)+1)*100)\n",
    "    print(\"Average AUROC: \",ROC_AVE*100)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f51536dbc085f2488cd4289a844cefb03f04dc07037b24fc5d73774403955e54"
  },
  "kernelspec": {
   "display_name": "tfp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
